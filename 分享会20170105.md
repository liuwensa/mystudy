## web性能优化
进行优化前，关键是剖析当前的web性能，找到性能瓶颈，从而确定最需改进的地方;
如果精力有限，首先将精力放在能明显提升性能的改进点上;

《高性能网站建设指南》提出了一个性能黄金法则：
只有10%-20%的最终用户响应时间花在了下载HTML文档上;其余的80%-90%的时间花在了下载页面中的所有组件上。

web性能对于用户体验有及其重要的影响，根据著名的2-5-8原则：

当用户在2秒以内得到响应，会感觉系统的响应非常快；
当用户在2-5秒之内得到响应，会感觉系统的响应速度还可以；
当用户在5-8秒之内得到响应，会感觉系统的响应非常慢，但还可以接受；
当用户在8秒之后都没有得到响应，会感觉系统糟透了，甚至系统已经挂掉；要么打开竞争对手的网站，要么重新发起第二次请求。

## 后台优化，启用静态页面，启用redis缓存;
服务端的接口优化，使用索引加速数据库查询，redis缓存
网站生成静态页面
公共的网站头部、尾部、侧边栏等写入静态页或者redis重复利用

## NGINX
开启Gzip压缩文件内容
如果应用运行在一台独立的服务器上，性能问题的解决方案很简单：换一台叼的服务器

但这可能不是问题所在，所以应该采用一种完全不同的方式，而不是升级硬件。

- 负载均衡：反向代理服务器上运行一个负载均衡器，把流量平均分配给一堆应用服务器。由于负载均衡器的引入，在增加应用服务器时可以完全不用修改应用程序。

- 缓存静态文件：直接请求的文件，比如图片或者代码文件，可以存在反向代理服务器上，并直接发送给客户端，这样可以更快地提供服务，分担了应用服务器的负载，可以让应用执行得更快。

- 保护网站 —— 反向代理服务器可以设置较高的安全级别，通过监控进快速识别和响应攻击，这样就可以把应用服务器保护起来。

NGINX软件是专门设计用做反向代理服务器的，NGINX通常被用于负载均衡，NGINX利用事件驱动处理的方法，比其它传统的服务器更加高效。

## 合并压缩
合并css、js等文件，更少的HTTP请求和单个文件解析都可以减少加载时间
压缩去除代码不必要的字符减少文件大小从而节省下载时间。
即使你用Gzip压缩过脚本和样式表，合并压缩这些文件仍然可以节省空间。


## 图片加载
图片用缩略图
懒加载、按需加载。

图片懒加载有着显著的三个好处：

- 减少向服务器发出的并发请求数量（这就使得页面的其他部分获得更快的加载时间）

- 减少浏览器的内存使用率（更少的图片，更少的内存）

- 减少服务器端的负载



# 敏感词过滤
敏感词过滤，一个很经典的需求场景，游信中贴吧、动态、聊天等需要敏感词过滤功能，之前的解决方案是，每次从redis中取出敏感词集合，然后做遍历操作，使用indexOf查看是否出现。

- DFA算法：trie存储，牺牲空间换效率；

- indexOf算法：自带算法

## DFA简介
敏感词过滤的算法中，DFA是唯一比较好的实现算法。DFA是确定有穷自动机，它是是通过event和当前的state得到下一个state，即event+state=nextstate。下图展示了其状态的转换
![](http://img.blog.csdn.net/20140525154027187?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2hlbnNzeQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

在这幅图中大写字母（S、U、V、Q）都是状态，小写字母a、b为动作。通过上图我们可以看到S+a=U,U+a=Q,S+b=V等等。

## 敏感词过滤

在文字过滤系统中，为了能够应付较高的并发，有一个目标比较重要，就是尽量的减少计算，而在DFA中，基本没有什么计算，有的只是状态的转移。而要把敏感词列表构造成一个状态机，用矩阵来实现是比较麻烦的，下面介绍一种比较简单的实现方式，就是树结构。 

首先我们对上图进行剖析。在这过程中我们认为下面这种结构会更加清晰明了
![](http://img.blog.csdn.net/20140525154009593?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2hlbnNzeQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
  同时这里没有状态转换，没有动作，有的只是查找（query）。我们可以认为，通过S query U、V，通过U query V、P，通过V query U P。通过这样的转变我们可以将状态的转换转变为使用集合的查找。

诚然，加入在我们的敏感词库中存在如下几个敏感词：日本人、日本鬼子。那么我需要构建成一个什么样的结构呢？
首先：query 日 ---> {本}、query 本 --->{人、鬼子、男人}、query 人 --->{null}、query 鬼 ---> {子}、query 男 ---> {人}。形如下结构：
![](http://i.imgur.com/jLIBe7n.png)

 这样我们就将我们的敏感词库构建成了一个类似与一颗一颗的树，这样我们判断一个词是否为敏感词时就大大减少了检索的匹配范围。比如我们要判断日本人，根据第一个字我们就可以确认需要检索的是那棵树，然后再在这棵树中进行检索。
 但是如何来判断一个敏感词已经结束了呢？利用标识位来判断。所以对于这个关键是如何来构建一棵棵这样的敏感词树。

![](http://i.imgur.com/yauyy0z.png)

检索

![](http://i.imgur.com/rRB8K7h.png)

当然你可以防盗搜索引擎中做，引擎的分词可以降低被误伤的概率

# 动态、贴吧缓存设计
1. hash，查出全部贴吧id列表放入redis缓存
2. Redis的zset结构做存储，天然有序，支持原子的增/删/查询操作。

动态实现分为：推（push）模式和拉（pull）模式

1. 推的模式： 
A发布动态之后，异步读取A的每个好友，并把这条动态插入到每个好友的动态列表。

	- 好处：读取效率高，可以看作O(1)的操作。
 
	- 坏处：写操作开销大，每秒1000条动态，每人N个好友，需要1000*N写操作，而且一个动态重复保存N次，产生大量冗余数据。随着用户产生数据的积累，长尾效应明显，冷数据占比会越来越高。而且redis对小zset采用压缩双链表（ziplist）的方式紧凑存储，列表增长会转换为跳跃表（skiplist），内存利用率下降。


2. 拉模式：根据用户的好友关系和个人动态列表，找到上次访问之后产生的新动态，增量实时聚合新内容。
 大致步骤：

	- 遍历我的好友，找到最近发表过动态的人

	- 遍历最近发表过动态，得到id和time

	- 合并到我的动态列表

聚合过程采用多线程并行执行，对查询性能影响很小。

拉模式，冷数据可以被淘汰删除，只缓存最近的热点数据，解决了存储成本高的问题。

# 附近的人
## 第一个版本：
最简单的方法就是遍历一遍，然后使用经纬度计算距离。计算公式是：
![](http://i.imgur.com/4TNCKC4.png)
r是地球半径，φ1, φ2是两点纬度，λ1, λ2是两点的经
这样的话，每次搜索附近的人，都可以通过公式计算出来附近x km的经纬度范围，然后去数据库查询。这样的缺点就是每次生成的sql语句都不一样，很难缓存，毕竟附近的人不是特别精确的，只要两个人在同一个范围内就可以认为是在一起的。

## 第二个版本，mongo的2D索引实现空间查询
```
find({location : {"$near" : [39.9937,116.4361]}}).sort({time:-1});
```
本来认为mongo自带索引应该效率会很好的，由于mongo的2D查询不能建立联合索引，按时间排序的话，性能比较低，超过100ms。通过数据文件挂载在内存盘上和按地理位置partition的方法，做了一些优化，效果还是不理想。并且很吃cpu，压力测试的时候8核cpu，吃了6核。

## 第三个版本，采用geohash算法实现了更高效的空间查询
geohash是一种地址编码，它能把二维的经纬度编码成一维的字符串。

geohash有以下几个特点：

1. geohash用一个字符串表示经度和纬度两个坐标。利用geohash，只需在一列上应用索引。
2. geohash表示的并不是一个点，而是一个矩形区域。比如编码wx4g08vdw9mg，它表示的是一个矩形区域。 发布该地址编码，既能表明自己位于某地点附近，又不至于暴露自己的精确坐标，有助于隐私保护。
3. 编码的前缀可以表示更大的区域。例如wx4g08vd，它的前缀wx4g08vd表示包含编码wx4g08vdw9mg在内的更大范围，可以用于附近地点搜索。首先根据用户当前坐标计算geohash然后取其前缀进行查询SELECT * FROM place WHERE geohash LIKE 'wx4g08vd%'，即可查询附近的所有地点。

## geohash的算法

### 根据经纬度计算GeoHash二进制编码

地球纬度区间是[-90,90]， 北海公园的纬度是39.928167，可以通过下面算法对纬度39.928167进行逼近编码:

1. 区间[-90,90]进行二分为[-90,0),[0,90]，称为左右区间，可以确定39.928167属于右区间[0,90]，给标记为1；
2. 接着将区间[0,90]进行二分为 [0,45),[45,90]，可以确定39.928167属于左区间 [0,45)，给标记为0；
3. 递归上述过程39.928167总是属于某个区间[a,b]。随着每次迭代区间[a,b]总在缩小，并越来越逼近39.928167；
4. 如果给定的纬度x（39.928167）属于左区间，则记录0，如果属于右区间则记录1，这样随着算法的进行会产生一个序列1011100，序列的长度跟给定的区间划分次数有关。

根据纬度算编码:
![](http://i.imgur.com/cATiUPA.png)

同理，地球经度区间是[-180,180]，可以对经度116.389550进行编码。

根据经度算编码:
![](http://i.imgur.com/k7tUqiR.png)

通过上述计算，纬度产生的编码为10111 00011，经度产生的编码为11010 01011。偶数位放经度，奇数位放纬度，把2串编码组合生成新串：11100 11101 00100 01111。

最后使用用0-9、b-z（去掉a, i, l, o）这32个字母进行base32编码，首先将11100 11101 00100 01111转成十进制，对应着28、29、4、15，十进制对应的编码就是wx4g。同理，将编码转换成经纬度的解码算法与之相反，具体不再赘述。
![](http://i.imgur.com/EewwwVe.png)

可以看出，当geohash base32编码长度为8时，精度在19米左右，而当编码长度为7时，精度在76米左右，编码长度需要根据数据情况进行选择。
![](http://i.imgur.com/kGku14f.png)

### GeoHash算法
![](http://i.imgur.com/llUXQdR.png)

如图所示，我们将二进制编码的结果填写到空间中，当将空间划分为四块时候，编码的顺序分别是左下角00，左上角01，右下脚10，右上角11，也就是类似于Z的曲线，当我们递归的将各个块分解成更小的子块时，编码的顺序是自相似的（分形），每一个子快也形成Z曲线，这种类型的曲线被称为Peano空间填充曲线。

这种类型的空间填充曲线的优点是将二维空间转换成一维曲线（事实上是分形维），对大部分而言，编码相似的距离也相近， 但Peano空间填充曲线最大的缺点就是突变性，有些编码相邻但距离却相差很远，比如0111与1000，编码是相邻的，但距离相差很大。
![](http://i.imgur.com/ETNZaVK.png)
除Peano空间填充曲线外，还有很多空间填充曲线，如图所示，其中效果公认较好是Hilbert空间填充曲线，相较于Peano曲线而言，Hilbert曲线没有较大的突变。为什么GeoHash不选择Hilbert空间填充曲线呢？可能是Peano曲线思路以及计算上比较简单吧，事实上，Peano曲线就是一种四叉树线性编码方式。

### geohash的应用
geohash的最大用途就是搜索附近的地址。不过，从geohash的编码算法中可以看出它的一个缺点：位于格子边界两侧的两点，虽然十分接近，但编码会完全不同。实际应用中，可以同时搜索当前格子周围的8个格子，即可解决这个问题。
```
SELECT * FROM `address` WHERE LEFT(`geohash`,5) IN ('wm6n0','wm6j8','wm6jc','wm3vz','wm3yp','wm6n1','wm6j9','wm3vx','wm6jb')
```

到这里，从数据库返回的结果，就是你附近的店，但是这时候并没有计算出实际距离与排序，还需要写代码计算实际距离并排序。

### ngeohash Geohash library for nodejs.
[git地址](https://github.com/liuwensa/node-geohash)
```
npm install ngeohash
```
```
const geohash = require('ngeohash');

console.log(geohash.encode(39.9069, 116.397));

const latlon = geohash.decode('wx4g08vdw9mg');
console.log(latlon.latitude);
console.log(latlon.longitude);

```